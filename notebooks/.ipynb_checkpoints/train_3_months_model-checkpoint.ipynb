{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ee1ece5-feb4-4abd-b2f9-d054f38e204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout,Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Dropout, Conv1D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from typing import Dict, Tuple\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e89f4ca-f7d4-4288-b0f4-e0d6f2a27f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB connection successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "mongo_url = os.getenv(\"MONGO_URL\")\n",
    "if not mongo_url:\n",
    "    raise ValueError(\"MONGO_URL not set in .env\")\n",
    "\n",
    "client = MongoClient(mongo_url, serverSelectionTimeoutMS=5000)  # 5-second timeout\n",
    "db = client[\"storage_simulation\"]\n",
    "collection = db[\"usage_logs\"]\n",
    "\n",
    "# Optional: test the connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"MongoDB connection successful\")\n",
    "except Exception as e:\n",
    "    print(f\"MongoDB connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bbdb974-b1fd-48b6-b1f1-bd746f13a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# here we we use 4 hour aggrigation so 7 per day\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "HORIZONS = {   \n",
    "    '3_month': 540,   # 30 days (6*30=180)  \n",
    "}\n",
    "SEQ_LENGTH = 42  # 7 days of historical data\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3e7bfb1-536e-41de-ad56-13ffc719fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data() -> Dict[str, dict]:\n",
    "    \"\"\"Load and preprocess data with proper feature engineering\"\"\"\n",
    "    raw_data = pd.DataFrame(list(collection.find()))\n",
    "    raw_data = raw_data.drop(columns=['_id'])\n",
    "    raw_data['timestamp'] = pd.to_datetime(raw_data['timestamp'])\n",
    "\n",
    "    print(\"\\nðŸ” Data Diagnostics:\")\n",
    "    print(f\"Total records: {len(raw_data)}\")\n",
    "    print(\"Unique directories:\", raw_data['directory'].unique())\n",
    "\n",
    "    processed = {}\n",
    "    for directory in raw_data['directory'].unique():\n",
    "        df = raw_data[raw_data['directory'] == directory].copy()\n",
    "        df = df.sort_values('timestamp').set_index('timestamp')\n",
    "\n",
    "        # Resample to 4-hour intervals\n",
    "        # The 'directory' column is excluded from the mean calculation\n",
    "        df = df[['storage_gb']].resample('4h').mean().ffill()\n",
    "\n",
    "        # Feature engineering\n",
    "        df['hour'] = df.index.hour\n",
    "        df['time_sin'] = np.sin(2 * np.pi * df.index.hour/23)\n",
    "        df['time_cos'] = np.cos(2 * np.pi * df.index.hour/23)\n",
    "\n",
    "        # Scale storage_gb\n",
    "        scaler = MinMaxScaler()\n",
    "        df['scaled_gb'] = scaler.fit_transform(df[['storage_gb']])\n",
    "\n",
    "        processed[directory] = {\n",
    "            'data': df[['scaled_gb', 'time_sin', 'time_cos']],\n",
    "            'original': df['storage_gb'],\n",
    "            'scaler': scaler\n",
    "        }\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14c31f32-4cde-418a-a3b0-e5ef97217051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(features: np.ndarray, targets: np.ndarray,\n",
    "                    seq_length: int, horizon: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create sequences with validation\"\"\"\n",
    "    X, y = [], []\n",
    "    max_start = len(features) - seq_length - horizon\n",
    "    if max_start < 0:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    for i in range(max_start + 1):\n",
    "        X.append(features[i:i+seq_length])\n",
    "        y.append(targets[i+seq_length:i+seq_length+horizon])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afd7eca4-a5f1-41d1-9d20-32240f00980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "def build_model(input_shape: Tuple[int, int], output_steps: int) -> Model:\n",
    "    \"\"\"Optimized forecasting model architecture\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Temporal pattern extraction\n",
    "    x = Conv1D(64, 3, activation='relu', padding='causal')(inputs)\n",
    "    x = GRU(128, return_sequences=True)(x)\n",
    "    x = GRU(64)(x)\n",
    "\n",
    "    # Prediction head\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(output_steps)(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbdfabe7-5c32-46ab-bd80-2d1bf06621b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "def save_model_and_scaler(model, scaler, name):\n",
    "    notebooks_dir = os.getcwd()\n",
    "\n",
    "    # Ensure name is only a base name, not a path\n",
    "    safe_name = os.path.basename(name)\n",
    "    safe_name = safe_name.replace('/', '_').replace('\\\\', '_')\n",
    "\n",
    "    # Create models directory\n",
    "    models_dir = os.path.join(notebooks_dir, 'models')\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    # Final model path\n",
    "    model_path = os.path.join(models_dir, f\"{safe_name}_3_monthly_forecast_model.keras\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved at: {model_path}\")\n",
    "\n",
    "    # Create scalers directory\n",
    "    scalers_dir = os.path.join(notebooks_dir, 'scalers')\n",
    "    os.makedirs(scalers_dir, exist_ok=True)\n",
    "\n",
    "    # Final scaler path\n",
    "    scaler_path = os.path.join(scalers_dir, f\"{safe_name}_3_monthly_scaler.pkl\")\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Scaler saved at: {scaler_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ad25b17-83f1-4f9b-8cbe-ad9ba88c3fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(data_dict: Dict) -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"Enhanced training with proper validation\"\"\"\n",
    "    modeles = {}\n",
    "    metrics = {}\n",
    "\n",
    "    for name, data in data_dict.items():\n",
    "        print(f\"\\nâš¡ Processing {name}\")\n",
    "        df = data['data']\n",
    "        scaler = data['scaler']\n",
    "\n",
    "        # Prepare data\n",
    "        total_points = len(df)\n",
    "        test_size = HORIZONS['3_month'] + SEQ_LENGTH\n",
    "        split_idx = total_points - test_size\n",
    "\n",
    "        if split_idx < SEQ_LENGTH:\n",
    "            print(f\"âš ï¸ Insufficient data for {name}\")\n",
    "            continue\n",
    "\n",
    "        # Create sequences\n",
    "        X_train, y_train = create_sequences(\n",
    "            df.values[:split_idx],\n",
    "            df['scaled_gb'].values[:split_idx],\n",
    "            SEQ_LENGTH, HORIZONS['3_month']\n",
    "        )\n",
    "        X_test, y_test = create_sequences(\n",
    "            df.values[split_idx:],\n",
    "            df['scaled_gb'].values[split_idx:],\n",
    "            SEQ_LENGTH, HORIZONS['3_month']\n",
    "        )\n",
    "\n",
    "        if len(X_train) == 0 or len(X_test) == 0:\n",
    "            print(f\"ðŸš« Sequence creation failed for {name}\")\n",
    "            continue\n",
    "\n",
    "        # Model setup\n",
    "        model = build_model((SEQ_LENGTH, 3), HORIZONS['3_month'])\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=[\n",
    "                EarlyStopping(patience=7, restore_best_weights=True),\n",
    "                ModelCheckpoint(f'best_{name}.keras', save_best_only=True)\n",
    "            ],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Generate predictions\n",
    "        test_pred = model.predict(X_test)\n",
    "        metrics[name] = {}\n",
    "\n",
    "        # Calculate metrics for each horizon\n",
    "        for horizon_name, steps in HORIZONS.items():\n",
    "            preds = test_pred[:, :steps].reshape(-1, 1)\n",
    "            true = y_test[:, :steps].reshape(-1, 1)\n",
    "\n",
    "            # Inverse transform predictions\n",
    "            preds_gb = scaler.inverse_transform(preds).reshape(-1, steps)\n",
    "            true_gb = scaler.inverse_transform(true).reshape(-1, steps)\n",
    "\n",
    "            # Calculate RMSE\n",
    "            rmse = np.sqrt(mean_squared_error(true_gb, preds_gb))\n",
    "\n",
    "            metrics[name][horizon_name] = {\n",
    "                'rmse': rmse,\n",
    "                'predictions': preds_gb[0],\n",
    "                'true': true_gb[0]\n",
    "            }\n",
    "            \n",
    "\n",
    "        modeles[name] = model\n",
    "\n",
    "        save_model_and_scaler(model, scaler, name)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    return modeles, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1f91369-1a6d-4eca-bbc3-8eda09811375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(name: str, data: dict, metrics: dict):\n",
    "    \"\"\"Enhanced plotting with actual dates\"\"\"\n",
    "    if name not in metrics:\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    original = data['original']\n",
    "\n",
    "    # Get test period dates\n",
    "    test_dates = original.index[-HORIZONS['3_month']:]\n",
    "\n",
    "    for horizon in HORIZONS:\n",
    "        if horizon not in metrics[name]:\n",
    "            continue\n",
    "\n",
    "        steps = HORIZONS[horizon]\n",
    "        preds = metrics[name][horizon]['predictions'][:steps]\n",
    "        true = metrics[name][horizon]['true'][:steps]\n",
    "        dates = test_dates[:steps]\n",
    "\n",
    "        plt.plot(dates, preds, label=f'{horizon} forecast')\n",
    "        plt.fill_between(dates,\n",
    "                        preds * 0.95,\n",
    "                        preds * 1.05,\n",
    "                        alpha=0.1)\n",
    "        plt.plot(dates, true, '--', label='Actual')\n",
    "\n",
    "    plt.title(f'{name} Storage Forecast')\n",
    "    plt.ylabel('Storage (GB)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ec6ee8-6830-4e2d-a14b-b8aa30b17f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Data Diagnostics:\n",
      "Total records: 292508\n",
      "Unique directories: ['/scratch' '/projects' '/customer' '/info']\n",
      "\n",
      "âš¡ Processing /scratch\n",
      "Epoch 1/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 450ms/step - loss: 0.2377 - val_loss: 0.1053\n",
      "Epoch 2/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 373ms/step - loss: 0.0848 - val_loss: 0.0413\n",
      "Epoch 3/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 451ms/step - loss: 0.0443 - val_loss: 0.0347\n",
      "Epoch 4/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 450ms/step - loss: 0.0362 - val_loss: 0.0348\n",
      "Epoch 5/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 375ms/step - loss: 0.0328 - val_loss: 0.0264\n",
      "Epoch 6/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 374ms/step - loss: 0.0309 - val_loss: 0.0308\n",
      "Epoch 7/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 439ms/step - loss: 0.0295 - val_loss: 0.0233\n",
      "Epoch 8/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 515ms/step - loss: 0.0290 - val_loss: 0.0211\n",
      "Epoch 9/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 372ms/step - loss: 0.0286 - val_loss: 0.0240\n",
      "Epoch 10/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 378ms/step - loss: 0.0277 - val_loss: 0.0246\n",
      "Epoch 11/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 457ms/step - loss: 0.0275 - val_loss: 0.0169\n",
      "Epoch 12/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 508ms/step - loss: 0.0263 - val_loss: 0.0132\n",
      "Epoch 13/50\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 401ms/step - loss: 0.0268 - val_loss: 0.0123\n",
      "Epoch 14/50\n",
      "\u001b[1m 1/14\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m6s\u001b[0m 479ms/step - loss: 0.0290"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_dict = load_and_preprocess_data()\n",
    "    models, metrics = train_and_evaluate(data_dict)\n",
    "\n",
    "    for directory in data_dict:\n",
    "        if directory in metrics:\n",
    "            print(f\"\\nðŸ“Š {directory.upper()} PERFORMANCE\")\n",
    "            for horizon in HORIZONS:\n",
    "                if horizon in metrics[directory]:\n",
    "                    rmse = metrics[directory][horizon]['rmse']\n",
    "                    print(f\"{horizon.replace('_', ' ').title():<12} RMSE: {rmse:.2f} GB\")\n",
    "            plot_results(directory, data_dict[directory], metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06558df-cb3c-4c30-a6dd-6858dd85f069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca63b71-4065-40e5-b8c0-50536a77a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout,Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Dropout, Conv1D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from typing import Dict, Tuple\n",
    "import pymongo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82f0c151-ff1d-401d-a3bc-fbfaf328ff9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB connection successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "mongo_url = os.getenv(\"MONGO_URL\")\n",
    "if not mongo_url:\n",
    "    raise ValueError(\"MONGO_URL not set in .env\")\n",
    "\n",
    "client = MongoClient(mongo_url, serverSelectionTimeoutMS=5000)  # 5-second timeout\n",
    "db = client[\"storage_simulation\"]\n",
    "collection = db[\"usage_logs\"]\n",
    "\n",
    "# Optional: test the connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"MongoDB connection successful\")\n",
    "except Exception as e:\n",
    "    print(f\"MongoDB connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdd20ab5-6be0-40d2-961e-5afadf67c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# here we we use 4 hour aggrigation so 7 per day\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "HORIZONS = {   \n",
    "    '1_week': 42,   # 7 days (6*7=42)  \n",
    "}\n",
    "SEQ_LENGTH = 42  # 7 days of historical data\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9a9dd74-5c7a-49c5-974e-bc840828b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data() -> Dict[str, dict]:\n",
    "    \"\"\"Load and preprocess data with proper feature engineering\"\"\"\n",
    "    raw_data = pd.DataFrame(list(collection.find()))\n",
    "    raw_data = raw_data.drop(columns=['_id'])\n",
    "    raw_data['timestamp'] = pd.to_datetime(raw_data['timestamp'])\n",
    "\n",
    "    print(\"\\nðŸ” Data Diagnostics:\")\n",
    "    print(f\"Total records: {len(raw_data)}\")\n",
    "    print(\"Unique directories:\", raw_data['directory'].unique())\n",
    "\n",
    "    processed = {}\n",
    "    for directory in raw_data['directory'].unique():\n",
    "        df = raw_data[raw_data['directory'] == directory].copy()\n",
    "        df = df.sort_values('timestamp').set_index('timestamp')\n",
    "\n",
    "        # Resample to 4-hour intervals\n",
    "        # The 'directory' column is excluded from the mean calculation\n",
    "        df = df[['storage_gb']].resample('4h').mean().ffill()\n",
    "\n",
    "        # Feature engineering\n",
    "        df['hour'] = df.index.hour\n",
    "        df['time_sin'] = np.sin(2 * np.pi * df.index.hour/23)\n",
    "        df['time_cos'] = np.cos(2 * np.pi * df.index.hour/23)\n",
    "\n",
    "        # Scale storage_gb\n",
    "        scaler = MinMaxScaler()\n",
    "        df['scaled_gb'] = scaler.fit_transform(df[['storage_gb']])\n",
    "\n",
    "        processed[directory] = {\n",
    "            'data': df[['scaled_gb', 'time_sin', 'time_cos']],\n",
    "            'original': df['storage_gb'],\n",
    "            'scaler': scaler\n",
    "        }\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d475e986-f6d6-49b8-99ac-186d6e81c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(features: np.ndarray, targets: np.ndarray,\n",
    "                    seq_length: int, horizon: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create sequences with validation\"\"\"\n",
    "    X, y = [], []\n",
    "    max_start = len(features) - seq_length - horizon\n",
    "    if max_start < 0:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    for i in range(max_start + 1):\n",
    "        X.append(features[i:i+seq_length])\n",
    "        y.append(targets[i+seq_length:i+seq_length+horizon])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e383da1-1c87-4b48-84de-a415b434b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "def build_model(input_shape: Tuple[int, int], output_steps: int) -> Model:\n",
    "    \"\"\"Optimized forecasting model architecture\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Temporal pattern extraction\n",
    "    x = Conv1D(64, 3, activation='relu', padding='causal')(inputs)\n",
    "    x = GRU(128, return_sequences=True)(x)\n",
    "    x = GRU(64)(x)\n",
    "\n",
    "    # Prediction head\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(output_steps)(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fe358ab-765e-4fe0-be39-5522ccb4e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "def save_model_and_scaler(model, scaler, name):\n",
    "    notebooks_dir = os.getcwd()\n",
    "\n",
    "    # Ensure name is only a base name, not a path\n",
    "    safe_name = os.path.basename(name)\n",
    "    safe_name = safe_name.replace('/', '_').replace('\\\\', '_')\n",
    "\n",
    "    # Create models directory\n",
    "    models_dir = os.path.join(notebooks_dir, 'models')\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    # Final model path\n",
    "    model_path = os.path.join(models_dir, f\"{safe_name}_weekly_forecast_model.keras\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved at: {model_path}\")\n",
    "\n",
    "    # Create scalers directory\n",
    "    scalers_dir = os.path.join(notebooks_dir, 'scalers')\n",
    "    os.makedirs(scalers_dir, exist_ok=True)\n",
    "\n",
    "    # Final scaler path\n",
    "    scaler_path = os.path.join(scalers_dir, f\"{safe_name}_weekly_scaler.pkl\")\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Scaler saved at: {scaler_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb5c46e6-18ff-4f95-bb98-e4bcdb8d5eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(data_dict: Dict) -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"Enhanced training with proper validation\"\"\"\n",
    "    modeles = {}\n",
    "    metrics = {}\n",
    "\n",
    "    for name, data in data_dict.items():\n",
    "        print(f\"\\nâš¡ Processing {name}\")\n",
    "        df = data['data']\n",
    "        scaler = data['scaler']\n",
    "\n",
    "        # Prepare data\n",
    "        total_points = len(df)\n",
    "        test_size = HORIZONS['1_week'] + SEQ_LENGTH\n",
    "        split_idx = total_points - test_size\n",
    "\n",
    "        if split_idx < SEQ_LENGTH:\n",
    "            print(f\"âš ï¸ Insufficient data for {name}\")\n",
    "            continue\n",
    "\n",
    "        # Create sequences\n",
    "        X_train, y_train = create_sequences(\n",
    "            df.values[:split_idx],\n",
    "            df['scaled_gb'].values[:split_idx],\n",
    "            SEQ_LENGTH, HORIZONS['1_week']\n",
    "        )\n",
    "        X_test, y_test = create_sequences(\n",
    "            df.values[split_idx:],\n",
    "            df['scaled_gb'].values[split_idx:],\n",
    "            SEQ_LENGTH, HORIZONS['1_week']\n",
    "        )\n",
    "\n",
    "        if len(X_train) == 0 or len(X_test) == 0:\n",
    "            print(f\"ðŸš« Sequence creation failed for {name}\")\n",
    "            continue\n",
    "\n",
    "        # Model setup\n",
    "        model = build_model((SEQ_LENGTH, 3), HORIZONS['1_week'])\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=[\n",
    "                EarlyStopping(patience=7, restore_best_weights=True),\n",
    "                ModelCheckpoint(f'best_{name}.keras', save_best_only=True)\n",
    "            ],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Generate predictions\n",
    "        test_pred = model.predict(X_test)\n",
    "        metrics[name] = {}\n",
    "\n",
    "        # Calculate metrics for each horizon\n",
    "        for horizon_name, steps in HORIZONS.items():\n",
    "            preds = test_pred[:, :steps].reshape(-1, 1)\n",
    "            true = y_test[:, :steps].reshape(-1, 1)\n",
    "\n",
    "            # Inverse transform predictions\n",
    "            preds_gb = scaler.inverse_transform(preds).reshape(-1, steps)\n",
    "            true_gb = scaler.inverse_transform(true).reshape(-1, steps)\n",
    "\n",
    "            # Calculate RMSE\n",
    "            rmse = np.sqrt(mean_squared_error(true_gb, preds_gb))\n",
    "\n",
    "            metrics[name][horizon_name] = {\n",
    "                'rmse': rmse,\n",
    "                'predictions': preds_gb[0],\n",
    "                'true': true_gb[0]\n",
    "            }\n",
    "\n",
    "        modeles[name] = model\n",
    "        save_model_and_scaler(model, scaler, name)\n",
    "\n",
    "    return modeles, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a78305d-bd4f-4082-b1ee-08b10daf4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(name: str, data: dict, metrics: dict):\n",
    "    \"\"\"Enhanced plotting with actual dates\"\"\"\n",
    "    if name not in metrics:\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    original = data['original']\n",
    "\n",
    "    # Get test period dates\n",
    "    test_dates = original.index[-HORIZONS['1_week']:]\n",
    "\n",
    "    for horizon in HORIZONS:\n",
    "        if horizon not in metrics[name]:\n",
    "            continue\n",
    "\n",
    "        steps = HORIZONS[horizon]\n",
    "        preds = metrics[name][horizon]['predictions'][:steps]\n",
    "        true = metrics[name][horizon]['true'][:steps]\n",
    "        dates = test_dates[:steps]\n",
    "\n",
    "        plt.plot(dates, preds, label=f'{horizon} forecast')\n",
    "        plt.fill_between(dates,\n",
    "                        preds * 0.95,\n",
    "                        preds * 1.05,\n",
    "                        alpha=0.1)\n",
    "        plt.plot(dates, true, '--', label='Actual')\n",
    "\n",
    "    plt.title(f'{name} Storage Forecast')\n",
    "    plt.ylabel('Storage (GB)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f6f9ce-bf44-470a-beec-e9078933fcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Data Diagnostics:\n",
      "Total records: 292504\n",
      "Unique directories: ['/scratch' '/projects' '/customer' '/info']\n",
      "\n",
      "âš¡ Processing /scratch\n",
      "Epoch 1/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 331ms/step - loss: 0.2116 - val_loss: 0.0355\n",
      "Epoch 2/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 398ms/step - loss: 0.0491 - val_loss: 0.0022\n",
      "Epoch 3/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 346ms/step - loss: 0.0244 - val_loss: 0.0029\n",
      "Epoch 4/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - loss: 0.0176 - val_loss: 0.0038\n",
      "Epoch 5/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - loss: 0.0149 - val_loss: 0.0019\n",
      "Epoch 6/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 387ms/step - loss: 0.0132 - val_loss: 0.0020\n",
      "Epoch 7/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 335ms/step - loss: 0.0116 - val_loss: 0.0011\n",
      "Epoch 8/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 285ms/step - loss: 0.0113 - val_loss: 0.0027\n",
      "Epoch 9/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 278ms/step - loss: 0.0107 - val_loss: 0.0020\n",
      "Epoch 10/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 368ms/step - loss: 0.0099 - val_loss: 0.0011\n",
      "Epoch 11/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 369ms/step - loss: 0.0093 - val_loss: 0.0012\n",
      "Epoch 12/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - loss: 0.0092 - val_loss: 0.0012\n",
      "Epoch 13/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - loss: 0.0091 - val_loss: 0.0012\n",
      "Epoch 14/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 387ms/step - loss: 0.0086 - val_loss: 0.0012\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608ms/step\n",
      "Model saved at: D:\\Projects\\HPE-StoragePrediction\\notebooks\\models\\scratch_weekly_forecast_model.keras\n",
      "Scaler saved at: D:\\Projects\\HPE-StoragePrediction\\notebooks\\scalers\\scratch_weekly_scaler.pkl\n",
      "\n",
      "âš¡ Processing /projects\n",
      "Epoch 1/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 404ms/step - loss: 0.2380 - val_loss: 0.0768\n",
      "Epoch 2/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 370ms/step - loss: 0.0562 - val_loss: 0.0018\n",
      "Epoch 3/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 503ms/step - loss: 0.0270 - val_loss: 0.0012\n",
      "Epoch 4/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 365ms/step - loss: 0.0187 - val_loss: 0.0031\n",
      "Epoch 5/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 372ms/step - loss: 0.0145 - val_loss: 5.0100e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 478ms/step - loss: 0.0127 - val_loss: 5.5182e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 408ms/step - loss: 0.0111 - val_loss: 0.0058\n",
      "Epoch 8/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 381ms/step - loss: 0.0104 - val_loss: 0.0040\n",
      "Epoch 9/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 448ms/step - loss: 0.0096 - val_loss: 5.2174e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m16/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 0.0091"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    data_dict = load_and_preprocess_data()\n",
    "    models, metrics = train_and_evaluate(data_dict)\n",
    "\n",
    "    for directory in data_dict:\n",
    "        if directory in metrics:\n",
    "            print(f\"\\nðŸ“Š {directory.upper()} PERFORMANCE\")\n",
    "            for horizon in HORIZONS:\n",
    "                if horizon in metrics[directory]:\n",
    "                    rmse = metrics[directory][horizon]['rmse']\n",
    "                    print(f\"{horizon.replace('_', ' ').title():<12} RMSE: {rmse:.2f} GB\")\n",
    "            plot_results(directory, data_dict[directory], metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b01a5-3f81-4eb0-8376-f249111dc1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
